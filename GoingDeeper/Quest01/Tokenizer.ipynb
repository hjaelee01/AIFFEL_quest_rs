{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42e44fc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n",
      "1.21.4\n",
      "0.5.2\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import konlpy\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "print(tf.__version__)\n",
    "print(np.__version__)\n",
    "print(konlpy.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88590741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.9/site-packages (0.1.96)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b49817ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Size: 94123\n",
      "Example:\n",
      ">> 개인용 컴퓨터 사용의 상당 부분은 \"이것보다 뛰어날 수 있느냐?\"\n",
      ">> 북한의 핵무기 계획을 포기하도록 하려는 압력이 거세지고 있는 가운데, 일본과 북한의 외교관들이 외교 관계를 정상화하려는 회담을 재개했다.\n",
      ">> \"경호 로보트가 침입자나 화재를 탐지하기 위해서 개인적으로, 그리고 전문적으로 사용되고 있습니다.\"\n",
      ">> 수자원부 당국은 논란이 되고 있고, 막대한 비용이 드는 이 사업에 대해 내년에 건설을 시작할 계획이다.\n",
      ">> 또한 근력 운동은 활발하게 걷는 것이나 최소한 20분 동안 뛰는 것과 같은 유산소 활동에서 얻는 운동 효과를 심장과 폐에 주지 않기 때문에, 연구학자들은 근력 운동이 심장에 큰 영향을 미치는지 여부에 대해 논쟁을 해왔다.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "path_to_file = os.getenv('HOME')+'/aiffel/sp_tokenizer/data/korean-english-park.train.ko'\n",
    "\n",
    "with open(path_to_file, \"r\") as f:\n",
    "    raw = f.read().splitlines()\n",
    "\n",
    "print(\"Data Size:\", len(raw))\n",
    "\n",
    "print(\"Example:\")\n",
    "for sen in raw[0:100][::20]: print(\">>\", sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8541ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장의 최단 길이: 1\n",
      "문장의 최장 길이: 377\n",
      "문장의 평균 길이: 60\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "min_len = 999\n",
    "max_len = 0\n",
    "sum_len = 0\n",
    "\n",
    "for sen in raw:\n",
    "    length = len(sen)\n",
    "    if min_len > length: min_len = length\n",
    "    if max_len < length: max_len = length\n",
    "    sum_len += length\n",
    "\n",
    "print(\"문장의 최단 길이:\", min_len)\n",
    "print(\"문장의 최장 길이:\", max_len)\n",
    "print(\"문장의 평균 길이:\", sum_len // len(raw))\n",
    "\n",
    "sentence_length = np.zeros((max_len), dtype=int)\n",
    "\n",
    "for sen in raw:\n",
    "    sentence_length[len(sen)-1] += 1\n",
    "\n",
    "plt.bar(range(max_len), sentence_length, width=1.0)\n",
    "plt.title(\"Sentence Length Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a75fef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Size: 77591\n",
      "문장의 최단 길이: 1\n",
      "문장의 최장 길이: 377\n",
      "문장의 평균 길이: 64\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "min_len = 999\n",
    "max_len = 0\n",
    "sum_len = 0\n",
    "\n",
    "cleaned_corpus = list(set(raw))  # set를 사용해서 중복을 제거합니다.\n",
    "print(\"Data Size:\", len(cleaned_corpus))\n",
    "\n",
    "for sen in cleaned_corpus:\n",
    "    length = len(sen)\n",
    "    if min_len > length: min_len = length\n",
    "    if max_len < length: max_len = length\n",
    "    sum_len += length\n",
    "\n",
    "print(\"문장의 최단 길이:\", min_len)\n",
    "print(\"문장의 최장 길이:\", max_len)\n",
    "print(\"문장의 평균 길이:\", sum_len // len(cleaned_corpus))\n",
    "\n",
    "sentence_length = np.zeros((max_len), dtype=int)\n",
    "\n",
    "for sen in cleaned_corpus:   # 중복이 제거된 코퍼스 기준\n",
    "    sentence_length[len(sen)-1] += 1\n",
    "\n",
    "plt.bar(range(max_len), sentence_length, width=1.0)\n",
    "plt.title(\"Sentence Length Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09bdaabc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_len = 150\n",
    "min_len = 10\n",
    "\n",
    "# 길이 조건에 맞는 문장만 선택합니다.\n",
    "filtered_corpus = [s for s in cleaned_corpus if (len(s) < max_len) & (len(s) >= min_len)]\n",
    "\n",
    "# 분포도를 다시 그려봅니다.\n",
    "sentence_length = np.zeros((max_len), dtype=int)\n",
    "\n",
    "for sen in filtered_corpus:\n",
    "    sentence_length[len(sen)-1] += 1\n",
    "\n",
    "plt.bar(range(max_len), sentence_length, width=1.0)\n",
    "plt.title(\"Sentence Length Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6890c3f",
   "metadata": {},
   "source": [
    "## SentencePiece Tokenizer 사용\n",
    "1. model_type = unnigram\n",
    "2. model_type = bpe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59944b5d",
   "metadata": {},
   "source": [
    "### model_type = unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29d49346",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: /aiffel/aiffel/sp_tokenizer/data/korean-english-park.train.ko.temp\n",
      "  input_format: \n",
      "  model_prefix: korean_spm\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 8000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(329) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(178) LOG(INFO) Loading corpus: /aiffel/aiffel/sp_tokenizer/data/korean-english-park.train.ko.temp\n",
      "trainer_interface.cc(385) LOG(INFO) Loaded all 76908 sentences\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(405) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(466) LOG(INFO) all chars count=4996369\n",
      "trainer_interface.cc(477) LOG(INFO) Done: 99.95% characters are covered.\n",
      "trainer_interface.cc(487) LOG(INFO) Alphabet size=1317\n",
      "trainer_interface.cc(488) LOG(INFO) Final character coverage=0.9995\n",
      "trainer_interface.cc(520) LOG(INFO) Done! preprocessed 76908 sentences.\n",
      "unigram_model_trainer.cc(139) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(143) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(194) LOG(INFO) Initialized 174340 seed sentencepieces\n",
      "trainer_interface.cc(526) LOG(INFO) Tokenizing input sentences with whitespace: 76908\n",
      "trainer_interface.cc(537) LOG(INFO) Done! 237965\n",
      "unigram_model_trainer.cc(489) LOG(INFO) Using 237965 sentences for EM training\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=92555 obj=14.853 num_tokens=523272 num_tokens/piece=5.65363\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=82083 obj=13.516 num_tokens=525776 num_tokens/piece=6.40542\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=61555 obj=13.5533 num_tokens=546907 num_tokens/piece=8.88485\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=61506 obj=13.5101 num_tokens=547350 num_tokens/piece=8.89913\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=46126 obj=13.6926 num_tokens=575369 num_tokens/piece=12.4739\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=46126 obj=13.6493 num_tokens=575466 num_tokens/piece=12.476\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=34594 obj=13.8894 num_tokens=606014 num_tokens/piece=17.5179\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=34594 obj=13.8387 num_tokens=606012 num_tokens/piece=17.5178\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=25945 obj=14.1301 num_tokens=637532 num_tokens/piece=24.5724\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=25945 obj=14.0747 num_tokens=637568 num_tokens/piece=24.5738\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=19458 obj=14.4091 num_tokens=670960 num_tokens/piece=34.4825\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=19458 obj=14.3468 num_tokens=670999 num_tokens/piece=34.4845\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=14593 obj=14.7196 num_tokens=705636 num_tokens/piece=48.3544\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=14593 obj=14.648 num_tokens=705645 num_tokens/piece=48.355\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=10944 obj=15.0875 num_tokens=741620 num_tokens/piece=67.765\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=10944 obj=15.007 num_tokens=741624 num_tokens/piece=67.7654\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=8800 obj=15.3757 num_tokens=769363 num_tokens/piece=87.4276\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=8800 obj=15.307 num_tokens=769367 num_tokens/piece=87.4281\n",
      "trainer_interface.cc(615) LOG(INFO) Saving model: korean_spm.model\n",
      "trainer_interface.cc(626) LOG(INFO) Saving vocabs: korean_spm.vocab\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "import os\n",
    "temp_file = os.getenv('HOME')+'/aiffel/sp_tokenizer/data/korean-english-park.train.ko.temp'\n",
    "\n",
    "vocab_size = 8000\n",
    "\n",
    "with open(temp_file, 'w') as f:\n",
    "    for row in filtered_corpus:   # 이전에 나왔던 정제했던 corpus를 활용해서 진행해야 합니다.\n",
    "        f.write(str(row) + '\\n')\n",
    "\n",
    "spm.SentencePieceTrainer.Train(\n",
    "    input=temp_file,\n",
    "    model_prefix='korean_spm',\n",
    "    vocab_size=vocab_size,\n",
    "    model_type='unigram'\n",
    ")\n",
    "# 위 Train에서  --model_type = unigram이 디폴트 적용되어 있습니다. --model_type = bpe로 옵션을 주어 변경할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a516454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 root root 376818 May 10 12:54 korean_spm.model\r\n",
      "-rw-r--r-- 1 root root 146213 May 10 12:54 korean_spm.vocab\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l korean_spm*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8554e6cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1243, 11, 302, 7, 3608, 11, 287, 38, 3]\n",
      "['▁아버지', '가', '방', '에', '들어', '가', '신', '다', '.']\n",
      "아버지가방에들어가신다.\n"
     ]
    }
   ],
   "source": [
    "s = spm.SentencePieceProcessor()\n",
    "s.Load('korean_spm.model')\n",
    "\n",
    "# SentencePiece를 활용한 sentence -> encoding\n",
    "tokensIDs = s.EncodeAsIds('아버지가방에들어가신다.')\n",
    "print(tokensIDs)\n",
    "\n",
    "# SentencePiece를 활용한 sentence -> encoded pieces\n",
    "print(s.SampleEncodeAsPieces('아버지가방에들어가신다.',1, 0.0))\n",
    "\n",
    "# SentencePiece를 활용한 encoding -> sentence 복원\n",
    "print(s.DecodeIds(tokensIDs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20569bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sp_tokenize(s, corpus): \n",
    "\n",
    "    tensor = []\n",
    "\n",
    "    for sen in corpus:\n",
    "        tensor.append(s.EncodeAsIds(sen))\n",
    "\n",
    "    with open(\"./korean_spm.vocab\", 'r') as f:\n",
    "        vocab = f.readlines()\n",
    "\n",
    "    word_index = {}\n",
    "    index_word = {}\n",
    "\n",
    "    for idx, line in enumerate(vocab):\n",
    "        word = line.split(\"\\t\")[0]\n",
    "\n",
    "        word_index.update({word:idx})\n",
    "        index_word.update({idx:word})\n",
    "\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='pre')\n",
    "\n",
    "    return tensor, word_index, index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "28ad4570",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9976970</td>\n",
       "      <td>아 더빙.. 진짜 짜증나네요 목소리</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3819312</td>\n",
       "      <td>흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10265843</td>\n",
       "      <td>너무재밓었다그래서보는것을추천한다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9045019</td>\n",
       "      <td>교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6483659</td>\n",
       "      <td>사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           document  label\n",
       "0   9976970                                아 더빙.. 진짜 짜증나네요 목소리      0\n",
       "1   3819312                  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나      1\n",
       "2  10265843                                  너무재밓었다그래서보는것을추천한다      0\n",
       "3   9045019                      교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정      0\n",
       "4   6483659  사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...      1"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_table('~/aiffel/sp_tokenizer/data/ratings_train.txt')\n",
    "test_data = pd.read_table('~/aiffel/sp_tokenizer/data/ratings_test.txt')\n",
    "\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49a1134a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sen):\n",
    "    # 텍스트에 영어가 있을 경우 소문자로 통일.\n",
    "    sen = str(sen).lower()\n",
    "    \n",
    "    # 불용어 제거\n",
    "    # ('ㅋ', '.', '~', '!', '?', '~', 'ㅠ', 'ㅜ', '♥', '^')\n",
    "    stopwords_list = ['ㅋ', '\\.', '~', '\\!', '\\?', 'ㅠ', 'ㅜ', '♥', '\\^\\^'] # 각 불용어를 리스트로 정의\n",
    "    stopwords_pattern = '|'.join(stopwords_list) # OR(|) 연산자로 연결하여 패턴 생성\n",
    "\n",
    "    # 정규표현식을 사용하여 불용어 제거\n",
    "    sen = re.sub(stopwords_pattern, ' ', sen)\n",
    "    \n",
    "    return sen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45b4409d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['document'] = data['document'].astype(str).apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cdad3d13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9976970</td>\n",
       "      <td>아 더빙   진짜 짜증나네요 목소리</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3819312</td>\n",
       "      <td>흠   포스터보고 초딩영화줄    오버연기조차 가볍지 않구나</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10265843</td>\n",
       "      <td>너무재밓었다그래서보는것을추천한다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9045019</td>\n",
       "      <td>교도소 이야기구먼   솔직히 재미는 없다  평점 조정</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6483659</td>\n",
       "      <td>사이몬페그의 익살스런 연기가 돋보였던 영화 스파이더맨에서 늙어보이기만 했던 커스틴 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           document  label\n",
       "0   9976970                                아 더빙   진짜 짜증나네요 목소리      0\n",
       "1   3819312                  흠   포스터보고 초딩영화줄    오버연기조차 가볍지 않구나      1\n",
       "2  10265843                                  너무재밓었다그래서보는것을추천한다      0\n",
       "3   9045019                      교도소 이야기구먼   솔직히 재미는 없다  평점 조정      0\n",
       "4   6483659  사이몬페그의 익살스런 연기가 돋보였던 영화 스파이더맨에서 늙어보이기만 했던 커스틴 ...      1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc73a34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data['document']의 모든 텍스트 토큰화\n",
    "tensor, word_index, index_word = sp_tokenize(s, data.iloc[:]['document'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "742b5e15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<unk>': 0,\n",
       " '<s>': 1,\n",
       " '</s>': 2,\n",
       " '.': 3,\n",
       " '▁': 4,\n",
       " '을': 5,\n",
       " '의': 6,\n",
       " '에': 7,\n",
       " '이': 8,\n",
       " '를': 9,\n",
       " '는': 10,\n",
       " '가': 11,\n",
       " '은': 12,\n",
       " ',': 13,\n",
       " '고': 14,\n",
       " '에서': 15,\n",
       " '로': 16,\n",
       " '한': 17,\n",
       " '▁“': 18,\n",
       " '인': 19,\n",
       " '”': 20,\n",
       " '일': 21,\n",
       " ')': 22,\n",
       " '(': 23,\n",
       " '과': 24,\n",
       " '▁이': 25,\n",
       " '와': 26,\n",
       " '으로': 27,\n",
       " '▁있다': 28,\n",
       " '지': 29,\n",
       " '도': 30,\n",
       " '▁수': 31,\n",
       " '할': 32,\n",
       " '했다': 33,\n",
       " '▁밝혔다': 34,\n",
       " '▁말했다': 35,\n",
       " '하고': 36,\n",
       " '년': 37,\n",
       " '다': 38,\n",
       " '하는': 39,\n",
       " '▁있는': 40,\n",
       " '기': 41,\n",
       " '리': 42,\n",
       " '▁그': 43,\n",
       " '자': 44,\n",
       " '▁전': 45,\n",
       " '며': 46,\n",
       " '스': 47,\n",
       " '해': 48,\n",
       " '▁2': 49,\n",
       " '▁그는': 50,\n",
       " '▁1': 51,\n",
       " '만': 52,\n",
       " '▁대한': 53,\n",
       " '된': 54,\n",
       " '▁위해': 55,\n",
       " '월': 56,\n",
       " '▁전했다': 57,\n",
       " '▁미국': 58,\n",
       " '▁한': 59,\n",
       " '▁미': 60,\n",
       " '▁3': 61,\n",
       " '▁이번': 62,\n",
       " '▁중': 63,\n",
       " '▁지난': 64,\n",
       " '현지시간': 65,\n",
       " '시': 66,\n",
       " '라': 67,\n",
       " '에게': 68,\n",
       " '나': 69,\n",
       " '▁대해': 70,\n",
       " '어': 71,\n",
       " '\"': 72,\n",
       " '사': 73,\n",
       " 's': 74,\n",
       " '주': 75,\n",
       " '▁것으로': 76,\n",
       " '▁‘': 77,\n",
       " '트': 78,\n",
       " '명이': 79,\n",
       " '▁것이라고': 80,\n",
       " '▁것': 81,\n",
       " '이라고': 82,\n",
       " '게': 83,\n",
       " '들이': 84,\n",
       " '▁\"': 85,\n",
       " '▁4': 86,\n",
       " '드': 87,\n",
       " '대': 88,\n",
       " '▁있다고': 89,\n",
       " '하기': 90,\n",
       " '르': 91,\n",
       " '수': 92,\n",
       " '▁것을': 93,\n",
       " '아': 94,\n",
       " '서': 95,\n",
       " '▁이라크': 96,\n",
       " '▁주': 97,\n",
       " '적인': 98,\n",
       " '▁6': 99,\n",
       " '하지': 100,\n",
       " '히': 101,\n",
       " '’': 102,\n",
       " '세': 103,\n",
       " '▁5': 104,\n",
       " '상': 105,\n",
       " '▁더': 106,\n",
       " '▁그러나': 107,\n",
       " '들은': 108,\n",
       " '부': 109,\n",
       " '하게': 110,\n",
       " '▁다른': 111,\n",
       " '▁대통령': 112,\n",
       " '운': 113,\n",
       " '적': 114,\n",
       " '▁중국': 115,\n",
       " '치': 116,\n",
       " '비': 117,\n",
       " 'e': 118,\n",
       " '명': 119,\n",
       " '▁한편': 120,\n",
       " '▁10': 121,\n",
       " '전': 122,\n",
       " '라고': 123,\n",
       " '던': 124,\n",
       " '성': 125,\n",
       " '▁북한': 126,\n",
       " '프': 127,\n",
       " '▁후': 128,\n",
       " '▁내': 129,\n",
       " '-': 130,\n",
       " '원': 131,\n",
       " '명의': 132,\n",
       " '소': 133,\n",
       " '됐다': 134,\n",
       " '▁바': 135,\n",
       " '미': 136,\n",
       " '정': 137,\n",
       " '▁이후': 138,\n",
       " '선': 139,\n",
       " '했다고': 140,\n",
       " '▁아': 141,\n",
       " '크': 142,\n",
       " '▁정부': 143,\n",
       " '▁영국': 144,\n",
       " '니': 145,\n",
       " '보': 146,\n",
       " '▁7': 147,\n",
       " '▁한국': 148,\n",
       " '▁통해': 149,\n",
       " '▁알': 150,\n",
       " '마': 151,\n",
       " '▁일': 152,\n",
       " '▁제': 153,\n",
       " '▁두': 154,\n",
       " '▁많은': 155,\n",
       " '장': 156,\n",
       " '▁또': 157,\n",
       " '이다': 158,\n",
       " '▁8': 159,\n",
       " '▁위한': 160,\n",
       " '될': 161,\n",
       " '▁대통령은': 162,\n",
       " '분': 163,\n",
       " '위': 164,\n",
       " '▁부시': 165,\n",
       " '진': 166,\n",
       " '화': 167,\n",
       " '야': 168,\n",
       " '▁등': 169,\n",
       " '하': 170,\n",
       " '▁가장': 171,\n",
       " '▁비': 172,\n",
       " '▁동안': 173,\n",
       " '까지': 174,\n",
       " '오': 175,\n",
       " '▁같은': 176,\n",
       " '▁것이다': 177,\n",
       " '제': 178,\n",
       " '▁대변인은': 179,\n",
       " '에는': 180,\n",
       " '▁대': 181,\n",
       " '당': 182,\n",
       " '바': 183,\n",
       " '조': 184,\n",
       " '면': 185,\n",
       " '국': 186,\n",
       " '개': 187,\n",
       " '▁지': 188,\n",
       " '▁유': 189,\n",
       " '디': 190,\n",
       " '들을': 191,\n",
       " '▁약': 192,\n",
       " '▁나': 193,\n",
       " '▁시': 194,\n",
       " '했으며': 195,\n",
       " '▁세계': 196,\n",
       " '파': 197,\n",
       " '하며': 198,\n",
       " '▁부': 199,\n",
       " '▁말': 200,\n",
       " '▁주장했다': 201,\n",
       " '▁문제': 202,\n",
       " '▁사': 203,\n",
       " '▁오바마': 204,\n",
       " '▁정': 205,\n",
       " '▁때': 206,\n",
       " '▁될': 207,\n",
       " '내': 208,\n",
       " '적으로': 209,\n",
       " '우': 210,\n",
       " '즈': 211,\n",
       " '▁것은': 212,\n",
       " '간': 213,\n",
       " '타': 214,\n",
       " '▁19': 215,\n",
       " '▁영화': 216,\n",
       " '▁할': 217,\n",
       " '▁한다': 218,\n",
       " '▁또한': 219,\n",
       " '▁뒤': 220,\n",
       " 'o': 221,\n",
       " '▁오': 222,\n",
       " '▁9': 223,\n",
       " '▁마': 224,\n",
       " '▁12': 225,\n",
       " '▁조사': 226,\n",
       " '▁현재': 227,\n",
       " '▁관련': 228,\n",
       " '▁것이': 229,\n",
       " '구': 230,\n",
       " '▁경제': 231,\n",
       " '▁최근': 232,\n",
       " '▁경우': 233,\n",
       " '▁보도했다': 234,\n",
       " '들': 235,\n",
       " '여': 236,\n",
       " '▁일본': 237,\n",
       " '▁덧붙였다': 238,\n",
       " '레': 239,\n",
       " '▁공격': 240,\n",
       " '▁이상': 241,\n",
       " '▁반': 242,\n",
       " '▁않았다': 243,\n",
       " 'A': 244,\n",
       " '▁있었다': 245,\n",
       " '▁무': 246,\n",
       " '및': 247,\n",
       " '차': 248,\n",
       " '한다': 249,\n",
       " '식': 250,\n",
       " '에서는': 251,\n",
       " '1': 252,\n",
       " '▁불': 253,\n",
       " '그': 254,\n",
       " '용': 255,\n",
       " 'a': 256,\n",
       " '단': 257,\n",
       " '러': 258,\n",
       " '▁국가': 259,\n",
       " 'd': 260,\n",
       " '려': 261,\n",
       " '했던': 262,\n",
       " '▁보': 263,\n",
       " '유': 264,\n",
       " \"'\": 265,\n",
       " '었다': 266,\n",
       " '권': 267,\n",
       " '문': 268,\n",
       " '▁11': 269,\n",
       " '약': 270,\n",
       " '▁기': 271,\n",
       " '▁20': 272,\n",
       " '▁일부': 273,\n",
       " '계': 274,\n",
       " '티': 275,\n",
       " '▁자신의': 276,\n",
       " '▁안': 277,\n",
       " '하다': 278,\n",
       " '▁해': 279,\n",
       " '▁거': 280,\n",
       " '2': 281,\n",
       " '▁공': 282,\n",
       " '관': 283,\n",
       " '▁함께': 284,\n",
       " '▁조': 285,\n",
       " '3': 286,\n",
       " '신': 287,\n",
       " '브': 288,\n",
       " '▁당시': 289,\n",
       " '모': 290,\n",
       " '▁했다': 291,\n",
       " '카': 292,\n",
       " '안': 293,\n",
       " \"▁'\": 294,\n",
       " '▁재': 295,\n",
       " '▁계속': 296,\n",
       " '▁새로': 297,\n",
       " '▁다': 298,\n",
       " '▁모든': 299,\n",
       " '▁파': 300,\n",
       " '▁그리고': 301,\n",
       " '방': 302,\n",
       " '▁후보': 303,\n",
       " '노': 304,\n",
       " '▁없다': 305,\n",
       " '▁하': 306,\n",
       " '회': 307,\n",
       " '실': 308,\n",
       " '▁때문에': 309,\n",
       " 'm': 310,\n",
       " '▁지원': 311,\n",
       " '직': 312,\n",
       " 'S': 313,\n",
       " '코': 314,\n",
       " '보다': 315,\n",
       " '해야': 316,\n",
       " '▁신': 317,\n",
       " '▁미군': 318,\n",
       " '▁파키스탄': 319,\n",
       " '무': 320,\n",
       " '▁예정이다': 321,\n",
       " '▁수도': 322,\n",
       " '린': 323,\n",
       " '▁상': 324,\n",
       " '억': 325,\n",
       " '▁프랑스': 326,\n",
       " '▁자': 327,\n",
       " '▁모두': 328,\n",
       " '호': 329,\n",
       " '▁있습니다': 330,\n",
       " '▁설명했다': 331,\n",
       " '▁몇': 332,\n",
       " '▁결과': 333,\n",
       " '▁러시아': 334,\n",
       " '▁계획': 335,\n",
       " '▁있으며': 336,\n",
       " '▁구': 337,\n",
       " '▁이스라엘': 338,\n",
       " '▁경찰': 339,\n",
       " '체': 340,\n",
       " '▁모': 341,\n",
       " '▁경찰은': 342,\n",
       " '산': 343,\n",
       " '점': 344,\n",
       " '토': 345,\n",
       " '▁지역': 346,\n",
       " '물': 347,\n",
       " '▁세': 348,\n",
       " '▁클린턴': 349,\n",
       " '▁카': 350,\n",
       " '▁유엔': 351,\n",
       " '질': 352,\n",
       " '▁정부는': 353,\n",
       " '▁민주당': 354,\n",
       " '경': 355,\n",
       " '동': 356,\n",
       " '▁테러': 357,\n",
       " '▁큰': 358,\n",
       " '▁도': 359,\n",
       " '▁피': 360,\n",
       " '▁우리': 361,\n",
       " 'C': 362,\n",
       " '법': 363,\n",
       " '▁사고': 364,\n",
       " '▁결정': 365,\n",
       " '▁선거': 366,\n",
       " 't': 367,\n",
       " '▁연구': 368,\n",
       " '들의': 369,\n",
       " '▁여성': 370,\n",
       " 'y': 371,\n",
       " '▁성': 372,\n",
       " '▁30': 373,\n",
       " '▁데': 374,\n",
       " '▁핵': 375,\n",
       " '피': 376,\n",
       " '더': 377,\n",
       " '▁군': 378,\n",
       " '명을': 379,\n",
       " '학': 380,\n",
       " '연': 381,\n",
       " '감': 382,\n",
       " 'u': 383,\n",
       " '▁지난해': 384,\n",
       " '되는': 385,\n",
       " 'c': 386,\n",
       " '에따르면': 387,\n",
       " '력': 388,\n",
       " '▁이들': 389,\n",
       " '▁시작': 390,\n",
       " '에도': 391,\n",
       " '▁강': 392,\n",
       " 'i': 393,\n",
       " '▁않을': 394,\n",
       " '▁열린': 395,\n",
       " '했습니다': 396,\n",
       " '▁줄': 397,\n",
       " '▁방문': 398,\n",
       " '▁발표': 399,\n",
       " '▁달러': 400,\n",
       " '군': 401,\n",
       " '▁당': 402,\n",
       " '▁경기': 403,\n",
       " '지만': 404,\n",
       " '▁대통령이': 405,\n",
       " '공': 406,\n",
       " '교': 407,\n",
       " '버': 408,\n",
       " '▁이란': 409,\n",
       " '▁달': 410,\n",
       " '▁올해': 411,\n",
       " '▁스': 412,\n",
       " '라는': 413,\n",
       " 'n': 414,\n",
       " '▁양': 415,\n",
       " '루': 416,\n",
       " '▁배': 417,\n",
       " '▁발생한': 418,\n",
       " '▁있을': 419,\n",
       " '▁이날': 420,\n",
       " '▁물': 421,\n",
       " '▁총리는': 422,\n",
       " '데': 423,\n",
       " '란': 424,\n",
       " '▁하는': 425,\n",
       " '▁원': 426,\n",
       " '▁소': 427,\n",
       " '행': 428,\n",
       " '▁the': 429,\n",
       " '▁노': 430,\n",
       " '키': 431,\n",
       " '▁발표했다': 432,\n",
       " '시간': 433,\n",
       " '중': 434,\n",
       " '▁15': 435,\n",
       " '심': 436,\n",
       " '재': 437,\n",
       " '▁리': 438,\n",
       " '▁초': 439,\n",
       " '6': 440,\n",
       " '▁발': 441,\n",
       " '▁받고': 442,\n",
       " '▁존': 443,\n",
       " '난': 444,\n",
       " 'D': 445,\n",
       " '살': 446,\n",
       " '▁실': 447,\n",
       " 'l': 448,\n",
       " '▁회담': 449,\n",
       " '이나': 450,\n",
       " '하면서': 451,\n",
       " '▁(': 452,\n",
       " '▁채': 453,\n",
       " '▁A': 454,\n",
       " '▁국제': 455,\n",
       " '▁총리': 456,\n",
       " '터': 457,\n",
       " '▁서': 458,\n",
       " '하여': 459,\n",
       " '▁유럽': 460,\n",
       " '▁총': 461,\n",
       " '▁관계': 462,\n",
       " '았다': 463,\n",
       " '되고': 464,\n",
       " '▁to': 465,\n",
       " '포': 466,\n",
       " '▁가운데': 467,\n",
       " '들에게': 468,\n",
       " '▁미국의': 469,\n",
       " '길': 470,\n",
       " 'P': 471,\n",
       " '▁아프간': 472,\n",
       " '▁S': 473,\n",
       " '▁라': 474,\n",
       " '으며': 475,\n",
       " '▁간': 476,\n",
       " '▁조치': 477,\n",
       " '▁타': 478,\n",
       " '▁기록': 479,\n",
       " '▁코': 480,\n",
       " '▁16': 481,\n",
       " '▁주장': 482,\n",
       " '종': 483,\n",
       " '▁첫': 484,\n",
       " '▁메': 485,\n",
       " '역': 486,\n",
       " '▁개': 487,\n",
       " '▁of': 488,\n",
       " 'M': 489,\n",
       " '▁포': 490,\n",
       " '▁의해': 491,\n",
       " '▁18': 492,\n",
       " '면서': 493,\n",
       " '▁아이': 494,\n",
       " '워': 495,\n",
       " '▁인도': 496,\n",
       " '▁된다': 497,\n",
       " '▁입': 498,\n",
       " '▁매': 499,\n",
       " '▁사람들이': 500,\n",
       " '▁주요': 501,\n",
       " '부터': 502,\n",
       " '▁사건': 503,\n",
       " '든': 504,\n",
       " '▁차': 505,\n",
       " '▁프로그램': 506,\n",
       " '.”': 507,\n",
       " '▁정부가': 508,\n",
       " '건': 509,\n",
       " '임': 510,\n",
       " '▁시위': 511,\n",
       " ')’': 512,\n",
       " '▁다시': 513,\n",
       " '요': 514,\n",
       " '▁감독': 515,\n",
       " '였다': 516,\n",
       " 'ing': 517,\n",
       " '5': 518,\n",
       " '▁인해': 519,\n",
       " 'er': 520,\n",
       " 'f': 521,\n",
       " '▁차량': 522,\n",
       " '▁승리': 523,\n",
       " '되어': 524,\n",
       " '▁보고': 525,\n",
       " '들과': 526,\n",
       " '입니다': 527,\n",
       " '형': 528,\n",
       " '▁동': 529,\n",
       " '▁회사': 530,\n",
       " '▁않은': 531,\n",
       " '설': 532,\n",
       " '▁열': 533,\n",
       " '▁13': 534,\n",
       " '▁투표': 535,\n",
       " '저': 536,\n",
       " '▁여': 537,\n",
       " '석': 538,\n",
       " 'ed': 539,\n",
       " '발': 540,\n",
       " '▁있다는': 541,\n",
       " '▁예정': 542,\n",
       " '거': 543,\n",
       " 'p': 544,\n",
       " '돼': 545,\n",
       " '▁피해': 546,\n",
       " '▁혐의로': 547,\n",
       " '7': 548,\n",
       " '▁어': 549,\n",
       " '▁우': 550,\n",
       " '▁하고': 551,\n",
       " '▁위': 552,\n",
       " '번': 553,\n",
       " '네': 554,\n",
       " '▁시간': 555,\n",
       " '▁활동': 556,\n",
       " '▁지지': 557,\n",
       " '▁혐의': 558,\n",
       " '▁외': 559,\n",
       " '▁반대': 560,\n",
       " '배': 561,\n",
       " 'g': 562,\n",
       " '금': 563,\n",
       " '▁중단': 564,\n",
       " '▁독일': 565,\n",
       " '▁협상': 566,\n",
       " '▁전쟁': 567,\n",
       " 'al': 568,\n",
       " '▁산': 569,\n",
       " '▁생각': 570,\n",
       " '반': 571,\n",
       " '두': 572,\n",
       " '▁합의': 573,\n",
       " '▁in': 574,\n",
       " '되지': 575,\n",
       " '품': 576,\n",
       " '장은': 577,\n",
       " '▁됐다': 578,\n",
       " '▁상원의원': 579,\n",
       " '▁시장': 580,\n",
       " '▁선': 581,\n",
       " '▁14': 582,\n",
       " '슨': 583,\n",
       " '%': 584,\n",
       " '너': 585,\n",
       " 'r': 586,\n",
       " '▁이미': 587,\n",
       " '민': 588,\n",
       " '▁대학': 589,\n",
       " '▁호주': 590,\n",
       " '래': 591,\n",
       " 'km': 592,\n",
       " '▁위치한': 593,\n",
       " '▁받았다': 594,\n",
       " '▁관리': 595,\n",
       " '▁대선': 596,\n",
       " '▁경': 597,\n",
       " '▁뉴욕': 598,\n",
       " '▁탈레반': 599,\n",
       " '료': 600,\n",
       " 'ar': 601,\n",
       " '▁우려': 602,\n",
       " '▁연': 603,\n",
       " '▁명': 604,\n",
       " '/': 605,\n",
       " '년간': 606,\n",
       " '▁팔레스타인': 607,\n",
       " '▁W': 608,\n",
       " '▁없는': 609,\n",
       " '▁호': 610,\n",
       " '됐다고': 611,\n",
       " '스트': 612,\n",
       " '달러': 613,\n",
       " '▁페': 614,\n",
       " '▁여러': 615,\n",
       " '병': 616,\n",
       " '▁우주': 617,\n",
       " '습니다': 618,\n",
       " '▁석방': 619,\n",
       " '론': 620,\n",
       " '달': 621,\n",
       " '▁최고': 622,\n",
       " 'I': 623,\n",
       " '▁아직': 624,\n",
       " '4': 625,\n",
       " '청': 626,\n",
       " '▁조지': 627,\n",
       " '▁내용': 628,\n",
       " '▁TV': 629,\n",
       " '0': 630,\n",
       " '▁새': 631,\n",
       " '▁소속': 632,\n",
       " '이라며': 633,\n",
       " '▁잘': 634,\n",
       " '▁추가': 635,\n",
       " 'an': 636,\n",
       " '로부터': 637,\n",
       " '▁연방': 638,\n",
       " '급': 639,\n",
       " '작': 640,\n",
       " '▁25': 641,\n",
       " '▁사실을': 642,\n",
       " '▁레': 643,\n",
       " '▁점': 644,\n",
       " 'E': 645,\n",
       " '▁위협': 646,\n",
       " '▁단': 647,\n",
       " '통': 648,\n",
       " '이며': 649,\n",
       " '▁매케인': 650,\n",
       " '▁관계자는': 651,\n",
       " '▁김': 652,\n",
       " '▁사용': 653,\n",
       " '속': 654,\n",
       " '▁남': 655,\n",
       " '▁않고': 656,\n",
       " '▁17': 657,\n",
       " '▁확인': 658,\n",
       " '▁포함': 659,\n",
       " 'F': 660,\n",
       " '지는': 661,\n",
       " '▁노력': 662,\n",
       " '다고': 663,\n",
       " '▁방송': 664,\n",
       " '▁많': 665,\n",
       " '릴': 666,\n",
       " '생': 667,\n",
       " '▁인터넷': 668,\n",
       " '판': 669,\n",
       " '▁지적했다': 670,\n",
       " '▁맞': 671,\n",
       " \"▁''\": 672,\n",
       " '▁이용': 673,\n",
       " '▁적': 674,\n",
       " '▁한다고': 675,\n",
       " '류': 676,\n",
       " '▁저': 677,\n",
       " '▁버': 678,\n",
       " '▁지금': 679,\n",
       " '▁도시': 680,\n",
       " '▁이슬람': 681,\n",
       " '▁남부': 682,\n",
       " '?': 683,\n",
       " '▁사이': 684,\n",
       " '스는': 685,\n",
       " '▁장': 686,\n",
       " '▁판매': 687,\n",
       " 'T': 688,\n",
       " '▁인근': 689,\n",
       " '▁비난': 690,\n",
       " '클': 691,\n",
       " '▁정치': 692,\n",
       " '▁매우': 693,\n",
       " '했었다': 694,\n",
       " '출': 695,\n",
       " '▁당국': 696,\n",
       " '▁폭발': 697,\n",
       " 'N': 698,\n",
       " '▁사람': 699,\n",
       " '▁올림픽': 700,\n",
       " '▁심': 701,\n",
       " '▁기술': 702,\n",
       " '천': 703,\n",
       " '▁하나': 704,\n",
       " '범': 705,\n",
       " '▁개발': 706,\n",
       " '▁보도': 707,\n",
       " '▁당국은': 708,\n",
       " '▁입장': 709,\n",
       " '▁대부분': 710,\n",
       " '▁볼': 711,\n",
       " '▁발견': 712,\n",
       " '츠': 713,\n",
       " '온': 714,\n",
       " 'or': 715,\n",
       " '▁사망': 716,\n",
       " '▁증가': 717,\n",
       " '했지만': 718,\n",
       " '메': 719,\n",
       " '▁부상': 720,\n",
       " '에따라': 721,\n",
       " '개월': 722,\n",
       " '했고': 723,\n",
       " '량': 724,\n",
       " '▁감': 725,\n",
       " '▁없다고': 726,\n",
       " '양': 727,\n",
       " '▁이어': 728,\n",
       " '된다': 729,\n",
       " '▁살': 730,\n",
       " '▁명의': 731,\n",
       " '▁B': 732,\n",
       " '▁올': 733,\n",
       " '▁정책': 734,\n",
       " '▁받은': 735,\n",
       " '▁자동차': 736,\n",
       " '▁2006': 737,\n",
       " '처럼': 738,\n",
       " '색': 739,\n",
       " 'K': 740,\n",
       " '▁않는다': 741,\n",
       " '▁손': 742,\n",
       " '▁못했다': 743,\n",
       " '▁이는': 744,\n",
       " '되었다': 745,\n",
       " '결': 746,\n",
       " '▁100': 747,\n",
       " '8': 748,\n",
       " '▁24': 749,\n",
       " '▁준비': 750,\n",
       " '퍼': 751,\n",
       " '증': 752,\n",
       " '▁M': 753,\n",
       " '위원회': 754,\n",
       " '▁루': 755,\n",
       " '▁교': 756,\n",
       " '강': 757,\n",
       " '▁C': 758,\n",
       " '▁날': 759,\n",
       " '▁남자': 760,\n",
       " '▁성명': 761,\n",
       " '▁어떤': 762,\n",
       " '▁생산': 763,\n",
       " '▁집': 764,\n",
       " '▁평화': 765,\n",
       " '▁발생': 766,\n",
       " '▁가족': 767,\n",
       " '▁오늘': 768,\n",
       " '베': 769,\n",
       " '편': 770,\n",
       " '했으나': 771,\n",
       " '▁금융': 772,\n",
       " '▁기자': 773,\n",
       " '▁건물': 774,\n",
       " '날': 775,\n",
       " '▁폭탄': 776,\n",
       " '친': 777,\n",
       " '▁하지만': 778,\n",
       " '▁21': 779,\n",
       " '▁이름': 780,\n",
       " '▁있던': 781,\n",
       " '울': 782,\n",
       " '▁본': 783,\n",
       " '▁베': 784,\n",
       " '▁오후': 785,\n",
       " '개의': 786,\n",
       " '하기로': 787,\n",
       " '▁지도자': 788,\n",
       " 'G': 789,\n",
       " '명은': 790,\n",
       " '이었다': 791,\n",
       " '▁해결': 792,\n",
       " '▁언론': 793,\n",
       " '▁P': 794,\n",
       " 'B': 795,\n",
       " '▁의사': 796,\n",
       " '▁50': 797,\n",
       " '▁방법': 798,\n",
       " 'en': 799,\n",
       " '▁검찰': 800,\n",
       " '▁23': 801,\n",
       " '하는데': 802,\n",
       " '머': 803,\n",
       " '▁27': 804,\n",
       " '▁미얀마': 805,\n",
       " 'h': 806,\n",
       " '▁자신': 807,\n",
       " '▁법원': 808,\n",
       " '▁변화': 809,\n",
       " '▁법': 810,\n",
       " '▁공화당': 811,\n",
       " '▁앞서': 812,\n",
       " '▁공개': 813,\n",
       " '▁대표': 814,\n",
       " '와의': 815,\n",
       " '▁행사': 816,\n",
       " '▁필요': 817,\n",
       " '▁상태': 818,\n",
       " '▁안전': 819,\n",
       " '처': 820,\n",
       " '▁토': 821,\n",
       " '영': 822,\n",
       " '▁북부': 823,\n",
       " '▁이에': 824,\n",
       " '▁22': 825,\n",
       " '립': 826,\n",
       " '▁있어': 827,\n",
       " '복': 828,\n",
       " '▁보호': 829,\n",
       " '▁방': 830,\n",
       " '▁분': 831,\n",
       " '▁종': 832,\n",
       " '▁치': 833,\n",
       " '▁건강': 834,\n",
       " '격': 835,\n",
       " '▁만': 836,\n",
       " '▁치료': 837,\n",
       " '▁인터뷰에서': 838,\n",
       " '▁병원': 839,\n",
       " '▁하지': 840,\n",
       " '절': 841,\n",
       " '▁컴퓨터': 842,\n",
       " '▁T': 843,\n",
       " ':': 844,\n",
       " '▁강조했다': 845,\n",
       " '▁다음': 846,\n",
       " '▁상승': 847,\n",
       " '합': 848,\n",
       " '기를': 849,\n",
       " '▁단체': 850,\n",
       " '테': 851,\n",
       " '▁갖고': 852,\n",
       " '▁아시아': 853,\n",
       " '▁이러': 854,\n",
       " 'b': 855,\n",
       " '▁그들은': 856,\n",
       " '▁살해': 857,\n",
       " '▁현': 858,\n",
       " '▁납치': 859,\n",
       " '▁처음': 860,\n",
       " '▁들어': 861,\n",
       " '각': 862,\n",
       " '예': 863,\n",
       " '는데': 864,\n",
       " '지역': 865,\n",
       " '겨': 866,\n",
       " '층': 867,\n",
       " '▁떨어진': 868,\n",
       " '▁것에': 869,\n",
       " '투': 870,\n",
       " '▁요구': 871,\n",
       " '▁학교': 872,\n",
       " '언': 873,\n",
       " '▁2005': 874,\n",
       " '▁추': 875,\n",
       " '▁힐러리': 876,\n",
       " '렌': 877,\n",
       " '▁원문기사보기': 878,\n",
       " '▁있도록': 879,\n",
       " '▁반군': 880,\n",
       " 'O': 881,\n",
       " '▁오전': 882,\n",
       " '하도록': 883,\n",
       " '▁사태': 884,\n",
       " '▁D': 885,\n",
       " '▁거래': 886,\n",
       " '▁마지막': 887,\n",
       " '▁조직': 888,\n",
       " 'in': 889,\n",
       " '탄': 890,\n",
       " '팀': 891,\n",
       " '▁여행': 892,\n",
       " '▁비난했다': 893,\n",
       " '져': 894,\n",
       " '▁논의': 895,\n",
       " '▁정보': 896,\n",
       " '▁터키': 897,\n",
       " '▁발언': 898,\n",
       " '▁금지': 899,\n",
       " '▁취': 900,\n",
       " '▁상황': 901,\n",
       " '▁북한이': 902,\n",
       " '▁가진': 903,\n",
       " '▁지구': 904,\n",
       " 'and': 905,\n",
       " '▁은행': 906,\n",
       " '▁경찰이': 907,\n",
       " '측': 908,\n",
       " '집': 909,\n",
       " '이라는': 910,\n",
       " '▁최대': 911,\n",
       " '▁진': 912,\n",
       " '▁팀': 913,\n",
       " '▁요청': 914,\n",
       " '▁오는': 915,\n",
       " '▁미국과': 916,\n",
       " '하자': 917,\n",
       " '업': 918,\n",
       " 'ic': 919,\n",
       " '▁F': 920,\n",
       " '▁처음으로': 921,\n",
       " '▁장관은': 922,\n",
       " 'on': 923,\n",
       " '거나': 924,\n",
       " '▁영': 925,\n",
       " '▁각': 926,\n",
       " '▁진행': 927,\n",
       " '▁자신이': 928,\n",
       " '▁연기': 929,\n",
       " '▁그녀는': 930,\n",
       " '▁그녀': 931,\n",
       " '▁이를': 932,\n",
       " '▁막': 933,\n",
       " '▁충돌': 934,\n",
       " '▁판결': 935,\n",
       " '▁출신': 936,\n",
       " '▁앞': 937,\n",
       " '▁온': 938,\n",
       " '으로부터': 939,\n",
       " 'L': 940,\n",
       " '▁사회': 941,\n",
       " '▁버락': 942,\n",
       " '▁규모': 943,\n",
       " 'R': 944,\n",
       " 'v': 945,\n",
       " '▁생': 946,\n",
       " '▁담': 947,\n",
       " '▁a': 948,\n",
       " '▁친': 949,\n",
       " '졌다': 950,\n",
       " '번째': 951,\n",
       " '▁지난달': 952,\n",
       " '됐으며': 953,\n",
       " '▁불법': 954,\n",
       " '▁용의자': 955,\n",
       " '열': 956,\n",
       " '▁없었다': 957,\n",
       " '▁26': 958,\n",
       " '▁행동': 959,\n",
       " '▁이탈리아': 960,\n",
       " '▁밤': 961,\n",
       " '▁최소': 962,\n",
       " '매': 963,\n",
       " '▁결': 964,\n",
       " '▁40': 965,\n",
       " '▁폴': 966,\n",
       " 'le': 967,\n",
       " '▁국민': 968,\n",
       " '▁보내': 969,\n",
       " '▁난': 970,\n",
       " '▁자금': 971,\n",
       " '▁성공': 972,\n",
       " '▁유지': 973,\n",
       " '▁앞으로': 974,\n",
       " '▁있지만': 975,\n",
       " '▁영향': 976,\n",
       " '▁야당': 977,\n",
       " '▁발사': 978,\n",
       " '▁번': 979,\n",
       " '▁없이': 980,\n",
       " '▁지역에서': 981,\n",
       " '▁공항': 982,\n",
       " '박': 983,\n",
       " '▁CNN': 984,\n",
       " '▁보인다': 985,\n",
       " 'w': 986,\n",
       " '▁투자': 987,\n",
       " '▁서울': 988,\n",
       " '▁된': 989,\n",
       " '▁들': 990,\n",
       " '▁̋': 991,\n",
       " '상을': 992,\n",
       " '등': 993,\n",
       " '▁H': 994,\n",
       " '먼': 995,\n",
       " '▁사망했다': 996,\n",
       " '▁예': 997,\n",
       " '▁주둔': 998,\n",
       " '▁관한': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "62a88849",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ...,  554,  514, 2648],\n",
       "       [   0,    0,    0, ..., 1311,  230,   69],\n",
       "       [   0,    0,    0, ..., 1011,  703,  249],\n",
       "       ...,\n",
       "       [   0,    0,    0, ...,    4, 1946,  278],\n",
       "       [   0,    0,    0, ...,  327,  167,  105],\n",
       "       [   0,    0,    0, ...,  947, 1086,  216]], dtype=int32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "016c97ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(train_data, test_data, s, num_words=10000):\n",
    "    # 중복 및 결측치 제거\n",
    "    train_data.drop_duplicates(subset=['document'], inplace=True)\n",
    "    train_data = train_data.dropna(how = 'any') \n",
    "    test_data.drop_duplicates(subset=['document'], inplace=True)\n",
    "    test_data = test_data.dropna(how = 'any') \n",
    "    \n",
    "    # 전처리\n",
    "    train_data['document'].astype(str).apply(preprocess)\n",
    "    test_data['document'].astype(str).apply(preprocess)\n",
    "    \n",
    "    # SentencePiece 사용해서 토큰화\n",
    "    train_tokens, word_to_index, index_to_word = sp_tokenize(s, train_data['document'])\n",
    "    test_tokens, _, _ = sp_tokenize(s, test_data['document'])\n",
    "    \n",
    "    return train_tokens, np.array(list(train_data['label'])), test_tokens, np.array(list(test_data['label'])), word_to_index\n",
    "    \n",
    "train_data = pd.read_table(os.getenv('HOME')+'/aiffel/sp_tokenizer/data/ratings_train.txt')\n",
    "test_data = pd.read_table(os.getenv('HOME')+'/aiffel/sp_tokenizer/data/ratings_test.txt')\n",
    "\n",
    "\n",
    "X_train, y_train, X_test, y_test, word_to_index = load_data(train_data, test_data, s) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0aa25640",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(146182, 140)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "42a11631",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49157, 140)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "922ea424",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a7b3f376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 16)          128000    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 8)                 800       \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 8)                 72        \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 128,881\n",
      "Trainable params: 128,881\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "word_vector_dim = 16\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model.add(tf.keras.layers.LSTM(8))\n",
    "model.add(tf.keras.layers.Dense(8, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "df6d9967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "229/229 [==============================] - 43s 11ms/step - loss: 0.6112 - accuracy: 0.7058 - val_loss: 0.4969 - val_accuracy: 0.7971\n",
      "Epoch 2/20\n",
      "229/229 [==============================] - 2s 8ms/step - loss: 0.4453 - accuracy: 0.8114 - val_loss: 0.4211 - val_accuracy: 0.8134\n",
      "Epoch 3/20\n",
      "229/229 [==============================] - 2s 8ms/step - loss: 0.3959 - accuracy: 0.8281 - val_loss: 0.4050 - val_accuracy: 0.8201\n",
      "Epoch 4/20\n",
      "229/229 [==============================] - 2s 8ms/step - loss: 0.3808 - accuracy: 0.8354 - val_loss: 0.4020 - val_accuracy: 0.8205\n",
      "Epoch 5/20\n",
      "229/229 [==============================] - 2s 8ms/step - loss: 0.3730 - accuracy: 0.8401 - val_loss: 0.3994 - val_accuracy: 0.8213\n",
      "Epoch 6/20\n",
      "229/229 [==============================] - 2s 8ms/step - loss: 0.3666 - accuracy: 0.8433 - val_loss: 0.3996 - val_accuracy: 0.8215\n",
      "Epoch 7/20\n",
      "229/229 [==============================] - 2s 9ms/step - loss: 0.3606 - accuracy: 0.8465 - val_loss: 0.3974 - val_accuracy: 0.8229\n",
      "Epoch 8/20\n",
      "229/229 [==============================] - 2s 8ms/step - loss: 0.3559 - accuracy: 0.8489 - val_loss: 0.3969 - val_accuracy: 0.8241\n",
      "Epoch 9/20\n",
      "229/229 [==============================] - 2s 9ms/step - loss: 0.3510 - accuracy: 0.8506 - val_loss: 0.3964 - val_accuracy: 0.8253\n",
      "Epoch 10/20\n",
      "229/229 [==============================] - 2s 9ms/step - loss: 0.3458 - accuracy: 0.8537 - val_loss: 0.3955 - val_accuracy: 0.8234\n",
      "Epoch 11/20\n",
      "229/229 [==============================] - 2s 9ms/step - loss: 0.3403 - accuracy: 0.8559 - val_loss: 0.3941 - val_accuracy: 0.8254\n",
      "Epoch 12/20\n",
      "229/229 [==============================] - 2s 9ms/step - loss: 0.3353 - accuracy: 0.8582 - val_loss: 0.3910 - val_accuracy: 0.8263\n",
      "Epoch 13/20\n",
      "229/229 [==============================] - 2s 8ms/step - loss: 0.3304 - accuracy: 0.8607 - val_loss: 0.3909 - val_accuracy: 0.8268\n",
      "Epoch 14/20\n",
      "229/229 [==============================] - 2s 9ms/step - loss: 0.3259 - accuracy: 0.8626 - val_loss: 0.3920 - val_accuracy: 0.8273\n",
      "Epoch 15/20\n",
      "229/229 [==============================] - 2s 9ms/step - loss: 0.3200 - accuracy: 0.8658 - val_loss: 0.3913 - val_accuracy: 0.8278\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0005)\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "epochs=20\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss',\n",
    "                           patience=2,\n",
    "                           restore_best_weights=True)\n",
    "checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "history = model.fit(X_train,\n",
    "                    y_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    verbose=1,\n",
    "                    callbacks=[early_stop, checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a89edcad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1537/1537 [==============================] - 5s 3ms/step - loss: 0.3922 - accuracy: 0.8270\n",
      "[0.3921646177768707, 0.8269625902175903]\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(X_test, y_test, verbose=1)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20c8c3c",
   "metadata": {},
   "source": [
    "### model_type=bpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6e937671",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: /aiffel/aiffel/sp_tokenizer/data/korean-english-park.train.ko.temp\n",
      "  input_format: \n",
      "  model_prefix: korean_spm\n",
      "  model_type: BPE\n",
      "  vocab_size: 8000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(329) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(178) LOG(INFO) Loading corpus: /aiffel/aiffel/sp_tokenizer/data/korean-english-park.train.ko.temp\n",
      "trainer_interface.cc(385) LOG(INFO) Loaded all 76908 sentences\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(405) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(466) LOG(INFO) all chars count=4996369\n",
      "trainer_interface.cc(477) LOG(INFO) Done: 99.95% characters are covered.\n",
      "trainer_interface.cc(487) LOG(INFO) Alphabet size=1317\n",
      "trainer_interface.cc(488) LOG(INFO) Final character coverage=0.9995\n",
      "trainer_interface.cc(520) LOG(INFO) Done! preprocessed 76908 sentences.\n",
      "trainer_interface.cc(526) LOG(INFO) Tokenizing input sentences with whitespace: 76908\n",
      "trainer_interface.cc(537) LOG(INFO) Done! 237965\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=37043 min_freq=188\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=10526 size=20 all=64102 active=6239 piece=▁가\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=7586 size=40 all=66979 active=9116 piece=▁일\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=5712 size=60 all=69013 active=11150 piece=▁하\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=4404 size=80 all=71006 active=13143 piece=▁해\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=3904 size=100 all=72666 active=14803 piece=▁결\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=3872 min_freq=147\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=3422 size=120 all=74308 active=5232 piece=▁반\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=3011 size=140 all=76157 active=7081 piece=들을\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=2743 size=160 all=77986 active=8910 piece=▁에\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=2451 size=180 all=79566 active=10490 piece=한다\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=2222 size=200 all=80983 active=11907 piece=에는\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=2209 min_freq=126\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=2027 size=220 all=82597 active=5345 piece=▁지역\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1878 size=240 all=83697 active=6445 piece=▁한편\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1732 size=260 all=85129 active=7877 piece=▁보도\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1623 size=280 all=86544 active=9292 piece=▁관련\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1557 size=300 all=87836 active=10584 piece=▁대통령은\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=1556 min_freq=113\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1510 size=320 all=89131 active=5686 piece=▁부시\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1411 size=340 all=90150 active=6705 piece=▁산\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1328 size=360 all=91505 active=8060 piece=▁계획\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1264 size=380 all=92481 active=9036 piece=▁한다\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1210 size=400 all=93606 active=10161 piece=▁했다\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=1208 min_freq=101\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1153 size=420 all=94570 active=5639 piece=▁토\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1104 size=440 all=95620 active=6689 piece=▁생각\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1050 size=460 all=96566 active=7635 piece=리스\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=993 size=480 all=97877 active=8946 piece=▁종\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=960 size=500 all=99285 active=10354 piece=▁수도\n",
      "bpe_model_trainer.cc(16"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "import os\n",
    "temp_file = os.getenv('HOME')+'/aiffel/sp_tokenizer/data/korean-english-park.train.ko.temp'\n",
    "\n",
    "vocab_size = 8000\n",
    "\n",
    "with open(temp_file, 'w') as f:\n",
    "    for row in filtered_corpus:   # 이전에 나왔던 정제했던 corpus를 활용해서 진행해야 합니다.\n",
    "        f.write(str(row) + '\\n')\n",
    "\n",
    "spm.SentencePieceTrainer.Train(\n",
    "    input=temp_file,\n",
    "    model_prefix='korean_spm',\n",
    "    vocab_size=vocab_size,\n",
    "    model_type='bpe'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cdf3acdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7) LOG(INFO) Updating active symbols. max_freq=959 min_freq=93\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=914 size=520 all=100009 active=5662 piece=▁지원\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=886 size=540 all=100963 active=6616 piece=▁채\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=863 size=560 all=101872 active=7525 piece=장이\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=835 size=580 all=102527 active=8180 piece=▁없다\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=809 size=600 all=103222 active=8875 piece=▁휴\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=808 min_freq=87\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=778 size=620 all=104234 active=6140 piece=▁나타\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=758 size=640 all=105000 active=6906 piece=▁a\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=739 size=660 all=106070 active=7976 piece=▁the\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=722 size=680 all=107085 active=8991 piece=▁K\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=706 size=700 all=107900 active=9806 piece=▁폐\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=705 min_freq=81\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=686 size=720 all=109102 active=6554 piece=▁각\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=664 size=740 all=109988 active=7440 piece=ing\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=651 size=760 all=111188 active=8640 piece=▁방송\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=627 size=780 all=111910 active=9362 piece=인을\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=611 size=800 all=112988 active=10440 piece=▁받아\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=610 min_freq=75\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=597 size=820 all=113726 active=6364 piece=▁의회\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=584 size=840 all=114502 active=7140 piece=▁2008\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=572 size=860 all=115558 active=8196 piece=▁골\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=553 size=880 all=116451 active=9089 piece=▁승리\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=540 size=900 all=117389 active=10027 piece=▁소속\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=540 min_freq=71\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=533 size=920 all=118207 active=6670 piece=▁사진\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=521 size=940 all=118926 active=7389 piece=▁24\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=508 size=960 all=119724 active=8187 piece=자들을\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=499 size=980 all=120621 active=9084 piece=▁보호\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=489 size=1000 all=121556 active=10019 piece=▁22\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=489 min_freq=67\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=475 size=1020 all=122136 active=6641 piece=대의\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=461 size=1040 all=123133 active=7638 piece=▁몰\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=453 size=1060 all=123897 active=8402 piece=▁오후\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=442 size=1080 all=124582 active=9087 piece=▁선수\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=430 size=1100 all=125174 active=9679 piece=카에\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=429 min_freq=63\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=423 size=1120 all=125908 active=6981 piece=▁시민\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=414 size=1140 all=126439 active=7512 piece=▁사태\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=406 size=1160 all=127264 active=8337 piece=국의\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=397 size=1180 all=128061 active=9134 piece=▁부인\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=393 size=1200 all=128731 active=9804 piece=▁백악관\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=393 min_freq=61\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=388 size=1220 all=129605 active=7298 piece=▁충돌\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=384 size=1240 all=130293 active=7986 piece=▁수감\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=377 size=1260 all=130992 active=8685 piece=합니다\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=371 size=1280 all=131855 active=9548 piece=▁이상의\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=365 size=1300 all=132534 active=10227 piece=▁이전\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=365 min_freq=58\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=360 size=1320 all=133153 active=7219 piece=▁사망했다\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=352 size=1340 all=133685 active=7751 piece=▁동영\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=346 size=1360 all=134204 active=8270 piece=스터\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=341 size=1380 all=134948 active=9014 piece=▁불구하고\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=335 size=1400 all=135581 active=9647 piece=▁지진\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=335 min_freq=55\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=328 size=1420 all=136186 active=7363 piece=▁놀\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=324 size=1440 all=136982 active=8159 piece=후보\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=320 size=1460 all=137580 active=8757 piece=▁서비\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=314 size=1480 all=138121 active=9298 piece=us\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=310 size=1500 all=138646 active=9823 piece=지구\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=310 min_freq=53\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=308 size=1520 all=139149 active=7402 piece=▁인한\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=305 size=1540 all=139874 active=8127 piece=▁계획을\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=300 size=1560 all=140342 active=8595 piece=▁이뤄\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=296 size=1580 all=140928 active=9181 piece=이를\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=291 size=1600 all=141753 active=10006 piece=차례\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=291 min_freq=51\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=287 size=1620 all=142342 active=7658 piece=▁윌리\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=283 size=1640 all=143021 active=8337 piece=▁선택\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=279 size=1660 all=143853 active=9169 piece=▁한국의\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=275 size=1680 all=144262 active=9578 piece=텍사\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=272 size=1700 all=144636 active=9952 piece=▁시장\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=272 min_freq=49\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=268 size=1720 all=145091 active=7650 piece=▁끌\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=264 size=1740 all=145496 active=8055 piece=ers\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=260 size=1760 all=146082 active=8641 piece=▁비상\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=256 size=1780 all=146726 active=9285 piece=▁샌\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=253 size=1800 all=147210 active=9769 piece=▁보였다\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=253 min_freq=48\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=250 size=1820 all=147710 active=7859 piece=▁사업\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=246 size=1840 all=148298 active=8447 piece=군의\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=244 size=1860 all=148901 active=9050 piece=▁한나라당\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=240 size=1880 all=149555 active=9704 piece=▁따라\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=237 size=1900 all=150341 active=10490 piece=으로써\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=237 min_freq=46\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=235 size=1920 all=150923 active=8056 piece=▁부모\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=233 size=1940 all=151336 active=8469 piece=▁의원은\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=229 size=1960 all=151810 active=8943 piece=▁사망자\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=226 size=1980 all=152221 active=9354 piece=▁특히\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=224 size=2000 all=152707 active=9840 piece=▁월요일\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=224 min_freq=45\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=221 size=2020 all=153338 active=8254 piece=▁되어\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=218 size=2040 all=153757 active=8673 piece=im\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=216 size=2060 all=154273 active=9189 piece=소에\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=213 size=2080 all=154856 active=9772 piece=운티\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=211 size=2100 all=155287 active=10203 piece=▁서로\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=211 min_freq=43\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=210 size=2120 all=155776 active=8245 piece=▁밝혀졌다\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=206 size=2140 all=156463 active=8932 piece=▁위반\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=204 size=2160 all=156912 active=9381 piece=▁동맹\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=203 size=2180 all=157599 active=10068 piece=책을\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=201 size=2200 all=158065 active=10534 piece=IA\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=201 min_freq=41\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=200 size=2220 all=158689 active=8505 piece=▁시즌\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=199 size=2240 all=159178 active=8994 piece=▁프로그램을\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=197 size=2260 all=159622 active=9438 piece=▁않아\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=195 size=2280 all=160134 active=9950 piece=명으로\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=193 size=2300 all=160657 active=10473 piece=▁80\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=193 min_freq=40\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=191 size=2320 all=160937 active=8303 piece=지에서\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=189 size=2340 all=161665 active=9031 piece=▁그룹\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=188 size=2360 all=162110 active=9476 piece=▁The\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=186 size=2380 all=162540 active=9906 piece=▁되고\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=184 size=2400 all=162933 active=10299 piece=니스\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=184 min_freq=39\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=182 size=2420 all=163449 active=8615 piece=▁수단\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=179 size=2440 all=163807 active=8973 piece=▁경영\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=178 size=2460 all=164193 active=9359 piece=▁아직까지\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=175 size=2480 all=164562 active=9728 piece=▁l\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=174 size=2500 all=165025 active=10191 piece=▁해야\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=174 min_freq=38\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=172 size=2520 all=165399 active=8617 piece=▁달러를\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=171 size=2540 all=165768 active=8986 piece=트에서\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=169 size=2560 all=166171 active=9389 piece=리로\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=168 size=2580 all=166772 active=9990 piece=▁도움을\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=166 size=2600 all=167173 active=10391 piece=▁능\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=166 min_freq=37\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=165 size=2620 all=167543 active=8720 piece=▁지시\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=163 size=2640 all=167892 active=9069 piece=▁론\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=162 size=2660 all=168369 active=9546 piece=▁힘들\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=161 size=2680 all=168824 active=10001 piece=▁con\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=159 size=2700 all=169192 active=10369 piece=▁오바마가\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=158 min_freq=36\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=157 size=2720 all=169585 active=8853 piece=▁어느\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=156 size=2740 all=170095 active=9363 piece=▁주의\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=155 size=2760 all=170547 active=9815 piece=▁비난을\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=153 size=2780 all=170886 active=10154 piece=BI\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=152 size=2800 all=171549 active=10817 piece=▁세금\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=152 min_freq=35\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=151 size=2820 all=171787 active=8801 piece=▁탁\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=150 size=2840 all=172258 active=9272 piece=▁넘는\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=149 size=2860 all=172657 active=9671 piece=▁목숨을\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=147 size=2880 all=173075 active=10089 piece=TO\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=146 size=2900 all=173440 active=10454 piece=▁거절\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=146 min_freq=34\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=145 size=2920 all=173766 active=8963 piece=▁불안\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=144 size=2940 all=174091 active=9288 piece=▁조종\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=143 size=2960 all=174672 active=9869 piece=이지만\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=142 size=2980 all=175260 active=10457 piece=▁동원\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=141 size=3000 all=175623 active=10820 piece=▁평양\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=141 min_freq=33\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=140 size=3020 all=175923 active=9063 piece=▁블로그\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=139 size=3040 all=176538 active=9678 piece=▁관련해\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=138 size=3060 all=176853 active=9993 piece=▁집단\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=137 size=3080 all=177344 active=10484 piece=▁병력\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=136 size=3100 all=177707 active=10847 piece=▁임무\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=136 min_freq=33\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=135 size=3120 all=178036 active=9191 piece=▁사무실\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=134 size=3140 all=178364 active=9519 piece=▁점을\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=133 size=3160 all=178730 active=9885 piece=▁크리스토\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=132 size=3180 all=179081 active=10236 piece=join\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=131 size=3200 all=179429 active=10584 piece=▁작년\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=131 min_freq=32\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=130 size=322"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0 all=179662 active=9202 piece=▁것도\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=129 size=3240 all=179961 active=9501 piece=분이\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=128 size=3260 all=180452 active=9992 piece=▁기본\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=127 size=3280 all=180817 active=10357 piece=▁저항\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=126 size=3300 all=181086 active=10626 piece=▁산불\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=126 min_freq=31\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=125 size=3320 all=181323 active=9283 piece=만의\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=125 size=3340 all=181793 active=9753 piece=▁부시는\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=124 size=3360 all=182129 active=10089 piece=▁연장\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=124 size=3380 all=182369 active=10329 piece=▁전망이다\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=123 size=3400 all=182764 active=10724 piece=▁허가\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=123 min_freq=31\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=122 size=3420 all=183104 active=9448 piece=▁사망했으며\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=121 size=3440 all=183434 active=9778 piece=▁대통령에게\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=119 size=3460 all=183674 active=10018 piece=//\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=119 size=3480 all=184031 active=10375 piece=▁기록을\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=118 size=3500 all=184229 active=10573 piece=소로\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=118 min_freq=30\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=118 size=3520 all=184718 active=9661 piece=▁경찰과\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=117 size=3540 all=185065 active=10008 piece=▁배럴\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=116 size=3560 all=185190 active=10133 piece=▁윤\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=116 size=3580 all=185598 active=10541 piece=▁지급\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=115 size=3600 all=185829 active=10772 piece=10\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=115 min_freq=29\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=114 size=3620 all=186179 active=9604 piece=▁당한\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=114 size=3640 all=186397 active=9822 piece=▁이들이\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=113 size=3660 all=186813 active=10238 piece=▁우크라이나\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=112 size=3680 all=187284 active=10709 piece=▁최소한\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=111 size=3700 all=187696 active=11121 piece=▁형태\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=111 min_freq=29\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=110 size=3720 all=187947 active=9623 piece=수도\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=110 size=3740 all=188147 active=9823 piece=▁테러를\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=109 size=3760 all=188467 active=10143 piece=▁자국\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=108 size=3780 all=188827 active=10503 piece=오와\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=107 size=3800 all=189268 active=10944 piece=비는\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=107 min_freq=28\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=107 size=3820 all=189535 active=9693 piece=▁준비가\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=106 size=3840 all=189828 active=9986 piece=관들은\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=105 size=3860 all=190151 active=10309 piece=ine\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=105 size=3880 all=190533 active=10691 piece=▁테러가\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=104 size=3900 all=190759 active=10917 piece=▁긴장\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=104 min_freq=28\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=104 size=3920 all=191140 active=9898 piece=▁위협을\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=103 size=3940 all=191654 active=10412 piece=▁와인\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=102 size=3960 all=191956 active=10714 piece=▁앓\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=102 size=3980 all=192281 active=11039 piece=▁않으면\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=101 size=4000 all=192494 active=11252 piece=▁내놓\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=101 min_freq=27\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=101 size=4020 all=192656 active=9768 piece=▁구체적인\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=100 size=4040 all=192950 active=10062 piece=▁나달\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=100 size=4060 all=193106 active=10218 piece=▁했으며\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=99 size=4080 all=193512 active=10624 piece=▁인정했다\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=98 size=4100 all=193891 active=11003 piece=▁불참\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=98 min_freq=27\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=98 size=4120 all=194124 active=9906 piece=▁테이프\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=97 size=4140 all=194366 active=10148 piece=▁음반\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=97 size=4160 all=194531 active=10313 piece=▁맨체스터\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=96 size=4180 all=194864 active=10646 piece=▁브루\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=96 size=4200 all=195072 active=10854 piece=▁조지아\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=96 min_freq=26\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=95 size=4220 all=195461 active=1"
     ]
    }
   ],
   "source": [
    "s.load('korean_spm.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "77deff1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1520, 1154, 6614, 6710, 2644, 6782, 6685, 6687]\n",
      "['▁아버', '지가', '방에', '들', '어가', '신', '다', '.']\n",
      "아버지가방에들어가신다.\n"
     ]
    }
   ],
   "source": [
    "# SentencePiece를 활용한 sentence -> encoding\n",
    "tokensIDs = s.EncodeAsIds('아버지가방에들어가신다.')\n",
    "print(tokensIDs)\n",
    "\n",
    "# SentencePiece를 활용한 sentence -> encoded pieces\n",
    "print(s.SampleEncodeAsPieces('아버지가방에들어가신다.',1, 0.0))\n",
    "\n",
    "# SentencePiece를 활용한 encoding -> sentence 복원\n",
    "print(s.DecodeIds(tokensIDs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4d4c0b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 16)          128000    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 8)                 800       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8)                 72        \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 128,881\n",
      "Trainable params: 128,881\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 모델 구조는 동일\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model.add(tf.keras.layers.LSTM(8))\n",
    "model.add(tf.keras.layers.Dense(8, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5ebb22b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "229/229 [==============================] - 4s 11ms/step - loss: 0.4796 - accuracy: 0.7766 - val_loss: 0.4130 - val_accuracy: 0.8149\n",
      "Epoch 2/20\n",
      "229/229 [==============================] - 2s 9ms/step - loss: 0.3911 - accuracy: 0.8300 - val_loss: 0.4035 - val_accuracy: 0.8185\n",
      "Epoch 3/20\n",
      "229/229 [==============================] - 2s 9ms/step - loss: 0.3779 - accuracy: 0.8365 - val_loss: 0.4022 - val_accuracy: 0.8207\n",
      "Epoch 4/20\n",
      "229/229 [==============================] - 2s 9ms/step - loss: 0.3702 - accuracy: 0.8405 - val_loss: 0.3986 - val_accuracy: 0.8207\n",
      "Epoch 5/20\n",
      "229/229 [==============================] - 2s 9ms/step - loss: 0.3640 - accuracy: 0.8441 - val_loss: 0.3958 - val_accuracy: 0.8229\n",
      "Epoch 6/20\n",
      "229/229 [==============================] - 2s 9ms/step - loss: 0.3588 - accuracy: 0.8459 - val_loss: 0.3949 - val_accuracy: 0.8233\n",
      "Epoch 7/20\n",
      "229/229 [==============================] - 2s 9ms/step - loss: 0.3533 - accuracy: 0.8486 - val_loss: 0.4056 - val_accuracy: 0.8190\n",
      "Epoch 8/20\n",
      "229/229 [==============================] - 2s 9ms/step - loss: 0.3487 - accuracy: 0.8506 - val_loss: 0.3925 - val_accuracy: 0.8249\n",
      "Epoch 9/20\n",
      "229/229 [==============================] - 2s 9ms/step - loss: 0.3431 - accuracy: 0.8531 - val_loss: 0.3919 - val_accuracy: 0.8257\n",
      "Epoch 10/20\n",
      "229/229 [==============================] - 2s 9ms/step - loss: 0.3383 - accuracy: 0.8551 - val_loss: 0.3901 - val_accuracy: 0.8259\n",
      "Epoch 11/20\n",
      "229/229 [==============================] - 2s 9ms/step - loss: 0.3325 - accuracy: 0.8572 - val_loss: 0.3902 - val_accuracy: 0.8260\n",
      "Epoch 12/20\n",
      "229/229 [==============================] - 2s 9ms/step - loss: 0.3278 - accuracy: 0.8598 - val_loss: 0.3883 - val_accuracy: 0.8275\n",
      "Epoch 13/20\n",
      "229/229 [==============================] - 2s 9ms/step - loss: 0.3227 - accuracy: 0.8621 - val_loss: 0.3956 - val_accuracy: 0.8268\n",
      "Epoch 14/20\n",
      "229/229 [==============================] - 2s 9ms/step - loss: 0.3176 - accuracy: 0.8637 - val_loss: 0.3921 - val_accuracy: 0.8260\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=optimizer,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "# 학습\n",
    "history = model.fit(X_train,\n",
    "                    y_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    verbose=1,\n",
    "                    callbacks=[early_stop, checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c5e78f",
   "metadata": {},
   "source": [
    "회고: sentencepiece trainer에서 model_type이 unigram일 때, bpe일 때 변화가 거의 없다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d407b330",
   "metadata": {},
   "source": [
    "## KoNLPY Tokenizer 사용\n",
    "1. Okt\n",
    "2. Hannanum\n",
    "3. Komoran"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8f0ea136",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "\n",
    "okt = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f3591534",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return ['/'.join(t) for t in okt.pos(text, norm=True, stem=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "569725c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization 함수화\n",
    "\n",
    "def konlpy_tokenize(tokenizer_instance, corpus, num_words=10000):\n",
    "    # 1. KoNLPy 토크나이저로 토큰화 (형태소 분석)\n",
    "    # Okt의 경우: okt.morphs(sentence, stem=True) 또는 okt.nouns(sentence) 등\n",
    "    # Hannanum의 경우: hannanum.morphs(sentence)\n",
    "    # Komoran의 경우: komoran.morphs(sentence)\n",
    "    # 여기서는 okt.morphs와 유사하게 일반적인 형태소 분석을 가정합니다.\n",
    "    # 각 토크나이저의 특성에 맞게 tokenize_fn을 호출 시점에 정의합니다.\n",
    "\n",
    "    tokenized_corpus = []\n",
    "    if isinstance(tokenizer_instance, Okt):\n",
    "        tokenize_fn = lambda text: tokenizer_instance.morphs(text, stem=True)\n",
    "    elif isinstance(tokenizer_instance, Hannanum):\n",
    "        tokenize_fn = lambda text: tokenizer_instance.morphs(text)\n",
    "    elif isinstance(tokenizer_instance, Komoran):\n",
    "        tokenize_fn = lambda text: tokenizer_instance.morphs(text)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported tokenizer type\")\n",
    "\n",
    "    for sen in corpus:\n",
    "        tokenized_corpus.append(tokenize_fn(str(sen)))\n",
    "        \n",
    "    # 2. Keras Tokenizer를 사용하여 단어 사전을 만들고 정수 인코딩\n",
    "    keras_tokenizer = Tokenizer(num_words=num_words, oov_token=\"<unk>\") # num_words는 충분히 큰 값으로 설정 가능\n",
    "    keras_tokenizer.fit_on_texts(tokenized_corpus)\n",
    "    \n",
    "    tensor = keras_tokenizer.texts_to_sequences(tokenized_corpus)\n",
    "\n",
    "    # 3. 패딩\n",
    "    tensor = pad_sequences(tensor, padding='pre', maxlen=X_train.shape[1] if 'X_train' in globals() else 140) # maxlen은 sentencepiece와 동일하게\n",
    "    word_index = keras_tokenizer.word_index\n",
    "    index_word = {idx: word for word, idx in word_index.items()}\n",
    "\n",
    "    return tensor, word_index, index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "aaab1a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_konlpy(train_data, test_data, konlpy_tokenizer_instance, num_words=10000):\n",
    "    # 중복 및 결측치 제거\n",
    "    train_data.drop_duplicates(subset=['document'], inplace=True)\n",
    "    train_data = train_data.dropna(how = 'any') \n",
    "    test_data.drop_duplicates(subset=['document'], inplace=True)\n",
    "    test_data = test_data.dropna(how = 'any') \n",
    "    \n",
    "    # 전처리 (기존 preprocess 함수 사용)\n",
    "    train_data['document'] = train_data['document'].astype(str).apply(preprocess)\n",
    "    test_data['document'] = test_data['document'].astype(str).apply(preprocess)\n",
    "    \n",
    "    # KoNLPy 토크나이저를 사용하여 토큰화 및 패딩\n",
    "    train_tokens, word_to_index, index_to_word = konlpy_tokenize(konlpy_tokenizer_instance, train_data['document'], num_words=num_words)\n",
    "    test_tokens, _, _ = konlpy_tokenize(konlpy_tokenizer_instance, test_data['document'], num_words=num_words) # 테스트 데이터는 학습된 word_to_index 사용\n",
    "    \n",
    "    # word_to_index에 num_words 제한 적용 (Keras Tokenizer에서 이미 처리하지만 명시적으로)\n",
    "    limited_word_to_index = {k: v for k, v in word_to_index.items() if v < num_words}\n",
    "    \n",
    "    return train_tokens, np.array(list(train_data['label'])), test_tokens, np.array(list(test_data['label'])), limited_word_to_index, index_to_word\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7dadb922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 정의 (SentencePiece와 동일한 구조 사용)\n",
    "def build_model(vocab_size, word_vector_dim=16):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "    model.add(tf.keras.layers.LSTM(8))\n",
    "    model.add(tf.keras.layers.Dense(8, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cfd6986a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습 및 평가 함수\n",
    "def train_and_evaluate_model(model, X_train_split, y_train_split, X_val_split, y_val_split, X_test_data, y_test_data, epochs=20, batch_size=512):\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.0005)\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "                  \n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
    "    # 각 토크나이저별 모델 저장 경로 다르게 설정\n",
    "    model_filename = f'best_model_{model.name}.h5' \n",
    "    checkpoint = ModelCheckpoint(model_filename, monitor='val_loss', save_best_only=True)\n",
    "\n",
    "    history = model.fit(X_train_split,\n",
    "                        y_train_split,\n",
    "                        epochs=epochs,\n",
    "                        batch_size=batch_size,\n",
    "                        validation_data=(X_val_split, y_val_split),\n",
    "                        verbose=1,\n",
    "                        callbacks=[early_stop, checkpoint])\n",
    "    \n",
    "    print(f\"======== {model.name} Test Results ========\")\n",
    "    results = model.evaluate(X_test_data, y_test_data, verbose=1)\n",
    "    print(f\"Loss: {results[0]}, Accuracy: {results[1]}\")\n",
    "    return history, results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1581f46a",
   "metadata": {},
   "source": [
    "### Okt 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f1451dc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_data_konlpy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_62/2312021736.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# 실제 KoNLPy 토크나이저의 단어 집합 크기는 Keras Tokenizer에 의해 결정됨\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m X_train_okt, y_train_okt, X_test_okt, y_test_okt, word_to_index_okt, index_to_word_okt = load_data_konlpy(\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mokt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load_data_konlpy' is not defined"
     ]
    }
   ],
   "source": [
    "# Okt를 사용하여 데이터 로드\n",
    "# num_words는 SentencePiece에서 사용한 vocab_size와 유사한 개념으로 사용\n",
    "# 실제 KoNLPy 토크나이저의 단어 집합 크기는 Keras Tokenizer에 의해 결정됨\n",
    "\n",
    "X_train_okt, y_train_okt, X_test_okt, y_test_okt, word_to_index_okt, index_to_word_okt = load_data_konlpy(\n",
    "    train_data.copy(), test_data.copy(), okt, num_words=vocab_size\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Okt Tokenizer\")\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_val shape:\", X_val.shape)\n",
    "print(\"X_test shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4e2928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Okt 모델 빌드\n",
    "model_okt = build_model(vocab_size=len(word_to_index_okt) + 1) # 실제 생성된 단어 수 + OOV\n",
    "model_okt._name = 'okt_model'\n",
    "model_okt.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b716198c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Okt 모델 학습 및 평가\n",
    "history_okt, results_okt = train_and_evaluate_model(\n",
    "    model_okt, X_train_okt, y_train_okt, X_val_okt, y_val_okt, X_test_okt, y_test_okt\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a631424",
   "metadata": {},
   "source": [
    "## 회고\n",
    "- SentencePiece Tokenizer에서 unigram과 bpe 사이의 유의미한 차이는 없었다.\n",
    "- Okt, Hannanum, Komoran도 비교해보고 싶었지만 시간이 예상보다 훨씬 오래 걸려 실험해보지 못한 점이 아쉬웠다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f996b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
